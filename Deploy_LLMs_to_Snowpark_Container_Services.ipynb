{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package Imports\n",
    "import dataiku\n",
    "from dataiku import pandasutils as pdu\n",
    "from dataiku.snowpark import DkuSnowpark\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import importlib\n",
    "import json\n",
    "import subprocess\n",
    "import requests\n",
    "import logging\n",
    "\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from snowflake.ml.registry import model_registry\n",
    "from snowflake.ml.model import deploy_platforms\n",
    "from snowflake.ml.model.models import huggingface_pipeline\n",
    "from snowflake.ml.model.models import llm\n",
    "from snowflake.ml.model import custom_model\n",
    "from snowflake.ml.model import model_signature\n",
    "\n",
    "from snowflake.snowpark.functions import lit\n",
    "import snowflake.connector\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params to control model name, num gpus, max tokens, etc.\n",
    "\n",
    "# The name of the Snowflake connection (in Dataiku Admin UI) that has access to deploy LLMs to the Snowflake Model Registry\n",
    "snowflake_connection_name = \"spcs-access-only\"\n",
    "\n",
    "# The name of the main LLM you want to deploy\n",
    "main_llm_model_name = \"ZEPHYRBeta\" # or \"LLAMA2\" or \"FALCON\" or \"ZEPHYRBeta\" or \"Phi2\"\n",
    "\n",
    "# SPCS compute pool to deploy the main LLM - note: must be set up on Snowflake side\n",
    "main_llm_compute_pool = \"DATAIKU_GPU_NV_S_MODEL_COMPUTE_POOL\"\n",
    "\n",
    "# The name of the embedding LLM you want to deploy\n",
    "embedding_llm_model_name = \"MiniLM-L6-v2\" \n",
    "\n",
    "# SPCS compute pool to deploy the embedding LLM - note: must be set up on Snowflake side\n",
    "embedding_llm_compute_pool = \"DATAIKU_CPU_X64_XS_EMBED_COMPUTE_POOL\"\n",
    "\n",
    "# Snowflake Model Registry DB and schema where you want to register the LLM\n",
    "database_name = \"DATAIKU_SPCS\"\n",
    "schema_name = \"MODEL_REGISTRY\"\n",
    "\n",
    "# Snowflake external access integration names\n",
    "external_access_integrations = [\"SNOWFLAKE_EGRESS_ACCESS_INTEGRATION\"]\n",
    "\n",
    "# Num GPUs available for main LLM\n",
    "main_llm_num_gpus = 1\n",
    "# Num SPCS container instances of the main LLM\n",
    "main_llm_max_instances = 1\n",
    "\n",
    "# Num GPUs available for embedding LLM\n",
    "embedding_llm_num_gpus = 0\n",
    "# Num SPCS container instances of the embedding LLM\n",
    "embedding_llm_max_instances = 1\n",
    "\n",
    "# Max new tokens for the main LLM to generate\n",
    "max_new_tokens = 200\n",
    "\n",
    "# Hugging face token\n",
    "#hugging_face_token = \"<YOUR_HF_TOKEN>\"\n",
    "\n",
    "hugging_face_token = dataiku.get_custom_variables()[\"hugging_face_token\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DSS wrapper around Snowpark\n",
    "dku_snowpark = DkuSnowpark()\n",
    "snowpark_session = dku_snowpark.get_session(snowflake_connection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_main_llm_to_spcs(modelname, snowparksession, computepool, maxnewtokens, numgpus, maxinstances, databasename, schemaname, hftoken, externalaccessintegrations):\n",
    "    \"\"\"\n",
    "    Deploy a chat complete LLM endpoint to SPCS using Snowpark ML\n",
    "    \"\"\"\n",
    "    \n",
    "    # Mapping of model name to the actual HuggingFace name, tokenizer (if required), and pip/conda dependencies\n",
    "    model_name_hf_mapping = {\n",
    "        \n",
    "        \"FALCON\": {\"hf_name\": \"tiiuae/falcon-7b-instruct\",\n",
    "                   \"tokenizer\": AutoTokenizer.from_pretrained(\"tiiuae/falcon-7b-instruct\"),\n",
    "                   \"token\": None,\n",
    "                   \"deployment_name\": \"falcon_7b_predict\",\n",
    "                   \"pip_requirements\": [\"einops\",\"snowflake-snowpark-python==1.9.0\"],\n",
    "                   \"conda_dependencies\": None},\n",
    "                \n",
    "        \"LLAMA2\": {\"hf_name\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "                   \"tokenizer\": None,\n",
    "                   \"token\": hftoken,\n",
    "                   \"deployment_name\": \"llama2_7b_predict\",\n",
    "                   \"pip_requirements\": None,\n",
    "                   \"conda_dependencies\": [\"snowflake-snowpark-python==1.9.0\"]},\n",
    "        \n",
    "        \"ZEPHYRBeta\": {\"hf_name\": \"HuggingFaceH4/zephyr-7b-beta\",\n",
    "                       \"tokenizer\": None,\n",
    "                       \"token\": None,\n",
    "                       \"deployment_name\": \"zephyr_beta_7b_predict\",\n",
    "                       \"pip_requirements\": None,\n",
    "                       \"conda_dependencies\": [\"snowflake-snowpark-python==1.9.0\", \"transformers==4.34.1\"]},\n",
    "        \n",
    "        \"Phi2\": {\"hf_name\": \"microsoft/phi-2\",\n",
    "                       \"tokenizer\": None,\n",
    "                       \"token\": None,\n",
    "                       \"deployment_name\": \"phi2_predict\",\n",
    "                       \"pip_requirements\": None,\n",
    "                       \"conda_dependencies\": [\"snowflake-snowpark-python==1.9.0\", \"transformers==4.37.1\"]}\n",
    "        \n",
    "    }\n",
    "    \n",
    "    hf_model_name = model_name_hf_mapping[modelname][\"hf_name\"]\n",
    "    tokenizer = model_name_hf_mapping[modelname][\"tokenizer\"]\n",
    "    token = model_name_hf_mapping[modelname][\"token\"]\n",
    "    \n",
    "    # Get a HuggingFace Pipeline Model  \n",
    "    hf_model = huggingface_pipeline.HuggingFacePipelineModel(task = \"text-generation\", \n",
    "                                                             model = hf_model_name, \n",
    "                                                             tokenizer = tokenizer, \n",
    "                                                             token = token,\n",
    "                                                             trust_remote_code = True, \n",
    "                                                             return_full_text = False, \n",
    "                                                             max_new_tokens = maxnewtokens,\n",
    "                                                             device_map = \"auto\",\n",
    "                                                             model_kwargs = {\"load_in_8bit\": True})\n",
    "    \n",
    "    # Get a Snowflake Model Registry (create if doesn't exist)\n",
    "    registry = model_registry.ModelRegistry(session = snowparksession,\n",
    "                                            database_name = databasename,\n",
    "                                            schema_name = schemaname,\n",
    "                                            create_if_not_exists = True)\n",
    "    \n",
    "    # Model version 1\n",
    "    model_version = \"1\"\n",
    "    \n",
    "    # Delete a previous version of the model if it already exists in the registry\n",
    "    try:\n",
    "        registry.delete_model(model_name = modelname,\n",
    "                              model_version = model_version)\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    # Log the model to the Snowflake Model Registry\n",
    "    hf_model_registry = registry.log_model(model_name = modelname,\n",
    "                                           model_version = model_version,\n",
    "                                           model = hf_model,\n",
    "                                           pip_requirements = model_name_hf_mapping[modelname][\"pip_requirements\"],\n",
    "                                           conda_dependencies = model_name_hf_mapping[modelname][\"conda_dependencies\"])\n",
    "    \n",
    "    # Deploy the model from the Registry to the SPCS compute pool chosen\n",
    "    deployed_model = hf_model_registry.deploy(deployment_name = model_name_hf_mapping[modelname][\"deployment_name\"],\n",
    "                              platform = deploy_platforms.TargetPlatform.SNOWPARK_CONTAINER_SERVICES,\n",
    "                              options={\"compute_pool\": computepool,\n",
    "                                       \"num_gpus\": numgpus, \n",
    "                                       \"max_instances\": maxinstances,\n",
    "                                       \"enable_ingress\": True,\n",
    "                                       \"external_access_integrations\": externalaccessintegrations\n",
    "                                      })\n",
    "            \n",
    "    print(\"Deployed Main LLM to SPCS\")\n",
    "    \n",
    "    return deployed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_embedding_llm_to_spcs(modelname, snowparksession, computepool, numgpus, maxinstances, databasename, schemaname, hftoken, externalaccessintegrations):\n",
    "    \"\"\"\n",
    "    Deploy a chat complete LLM endpoint to SPCS using Snowpark ML\n",
    "    \"\"\"\n",
    "    \n",
    "    # Mapping of model name (only 1 written right now) to the actual HuggingFace name and pip/conda dependencies\n",
    "    model_name_hf_mapping = {\n",
    "        \"MiniLM-L6-v2\": {\"hf_name\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "                         \"deployment_name\": \"MiniLM_L6_v2_embed\",\n",
    "                         \"pip_requirements\": None,\n",
    "                         \"conda_dependencies\": [\"snowflake-snowpark-python==1.9.0\", \"transformers==4.34.1\", \n",
    "                                                \"sentence-transformers==2.2.2\", \"langchain\"]\n",
    "                        }\n",
    "    }\n",
    "    \n",
    "    if modelname == \"MiniLM-L6-v2\":\n",
    "        hf_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    \n",
    "    # Create a custom Embedding LLM Class to wrap around the LLM and conform it to SPCS standards\n",
    "    class EmbeddingLLMCustom(custom_model.CustomModel):\n",
    "        def __init__(self, context: custom_model.ModelContext) -> None:\n",
    "            super().__init__(context)\n",
    "\n",
    "            self.embeddings = HuggingFaceEmbeddings(\n",
    "                model_name = model_name_hf_mapping[modelname][\"hf_name\"],\n",
    "            )\n",
    "\n",
    "        @custom_model.inference_api\n",
    "        def predict(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "            def _embed(input_text: str) -> str:\n",
    "\n",
    "                query_result = self.embeddings.embed_query(input_text)\n",
    "\n",
    "                return str(query_result)\n",
    "\n",
    "            res_df = pd.DataFrame({\"outputs\": pd.Series.apply(X[\"inputs\"], _embed)})\n",
    "            return res_df\n",
    "    \n",
    "    # Get the custom LLM Class\n",
    "    hf_model = EmbeddingLLMCustom(custom_model.ModelContext())   \n",
    "        \n",
    "    # Get a Snowflake Model Registry (create if doesn't exist)\n",
    "    registry = model_registry.ModelRegistry(session = snowparksession,\n",
    "                                            database_name = databasename,\n",
    "                                            schema_name = schemaname,\n",
    "                                            create_if_not_exists = True)\n",
    "    \n",
    "    # Model version 1\n",
    "    model_version = \"1\"\n",
    "    \n",
    "    # Delete a previous version of the model if it already exists in the registry\n",
    "    try:\n",
    "        registry.delete_model(model_name = modelname,\n",
    "                              model_version = model_version)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Log the model to the Snowflake Model Registry\n",
    "    hf_model_registry = registry.log_model(\n",
    "        model_name = modelname,\n",
    "        model_version = model_version,\n",
    "        model = hf_model,\n",
    "        conda_dependencies = model_name_hf_mapping[modelname][\"conda_dependencies\"],\n",
    "        signatures = {\n",
    "            \"predict\": model_signature.ModelSignature(\n",
    "                inputs = [model_signature.FeatureSpec(name=\"inputs\", dtype=model_signature.DataType.STRING)],\n",
    "                outputs = [model_signature.FeatureSpec(name=\"outputs\", dtype=model_signature.DataType.STRING)],\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Deploy the model from the Registry to the SPCS compute pool chosen\n",
    "    deployed_model = hf_model_registry.deploy(deployment_name = model_name_hf_mapping[modelname][\"deployment_name\"],\n",
    "                                              platform = deploy_platforms.TargetPlatform.SNOWPARK_CONTAINER_SERVICES,\n",
    "                                              options={\"compute_pool\": computepool,\n",
    "                                                       \"max_instances\": maxinstances,\n",
    "                                                       \"num_gpus\": numgpus,\n",
    "                                                       \"enable_ingress\": True,\n",
    "                                                       \"external_access_integrations\": externalaccessintegrations\n",
    "                                                      })\n",
    "    \n",
    "    print(\"Deployed Embedding LLM to SPCS\")\n",
    "    \n",
    "    return deployed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the main LLM to SPCS\n",
    "main_llm_on_spcs = deploy_main_llm_to_spcs(main_llm_model_name, snowpark_session, main_llm_compute_pool, \n",
    "                                           max_new_tokens, main_llm_num_gpus, main_llm_max_instances,\n",
    "                                           database_name, schema_name, hugging_face_token, external_access_integrations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the embedding LLM to SPCS\n",
    "embedding_llm_on_spcs = deploy_embedding_llm_to_spcs(embedding_llm_model_name, snowpark_session, \n",
    "                                                     embedding_llm_compute_pool, embedding_llm_num_gpus, \n",
    "                                                     embedding_llm_max_instances, database_name, schema_name, \n",
    "                                                     hugging_face_token, external_access_integrations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the public URLs of the main and embedding LLMs on SPCS\n",
    "def get_llm_endpoint_urls(snowparksession, main_llm, embedding_llm):\n",
    "    \n",
    "    try:\n",
    "        main_llm_service_name = main_llm['details']['service_info']['name']\n",
    "        show_services_query = \"SHOW ENDPOINTS IN SERVICE \" + main_llm_service_name\n",
    "        show_services_query_result = snowparksession.sql(show_services_query)\n",
    "        service = show_services_query_result.collect()[0]\n",
    "        spcs_service_url = service['ingress_url']\n",
    "        spcs_service_url_full = \"https://\" + spcs_service_url\n",
    "        print(\"Chat completion LLM URL: \" + spcs_service_url_full)\n",
    "    except:\n",
    "        print(\"No chat complete URL. Can take a minute to generate the public URL...try again\")\n",
    "         \n",
    "    try:\n",
    "        embedding_llm_service_name = embedding_llm['details']['service_info']['name']\n",
    "        show_services_query = \"SHOW ENDPOINTS IN SERVICE \" + embedding_llm_service_name\n",
    "        show_services_query_result = snowparksession.sql(show_services_query)\n",
    "        service = show_services_query_result.collect()[0]\n",
    "        spcs_service_url = service['ingress_url']\n",
    "        spcs_service_url_full = \"https://\" + spcs_service_url\n",
    "        print(\"Text embedding LLM URL: \" + spcs_service_url_full)\n",
    "    except:\n",
    "        print(\"No text embedding URL. Can take a minute to generate the public URL...try again\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the LLM endpoint URLS\n",
    "get_llm_endpoint_urls(snowpark_session, main_llm_on_spcs, embedding_llm_on_spcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "createdOn": 1695053827986,
  "creator": "pat",
  "customFields": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python (env py_38_snowpark_llms)",
   "language": "python",
   "name": "py-dku-venv-py_38_snowpark_llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "modifiedBy": "pat",
  "tags": []
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
